# ğŸ‘ï¸ Eye-Controlled Smart Car with Turkish Voice Feedback

A real-time deep learning system that detects eye movement direction and controls a robot car using Raspberry Pi via socket communication. The system uses a custom-trained CNN model to classify gaze direction (`left`, `right`, `forward`, `stop`) and provides Turkish voice feedback.

---

## ğŸ¯ Project Goal

To create a hands-free car control system using only eye movements, enabling an assistive or intelligent robotic interface that responds with physical movement and speech.

---

## ğŸ§  How It Works

1. A CNN model is trained on webcam images to recognize eye direction.
2. The trained model is used on a Mac to detect gaze in real time.
3. Detected direction is sent via socket to a Raspberry Pi.
4. The Raspberry Pi controls motors accordingly and plays a Turkish voice response using gTTS and a speaker.

---

## ğŸ—‚ï¸ Dataset Structure

The `Data/` folder contains training images for four eye directions. Each class is stored in its own subfolder:

/Data

â”œâ”€â”€ Data_Dur # Stop

â”œâ”€â”€ Data_Duz # Move forward

â”œâ”€â”€ Data_Sag # Turn right

â””â”€â”€ Data_Sol # Turn left

Each folder includes images of the user's eye in the corresponding gaze direction. These images are used to train a deep learning model with TensorFlow/Keras.

---

## ğŸ§  Model Training

The CNN model is trained using the following configuration:

- **Input size:** 100x100 pixels RGB
- **Architecture:** Conv2D â†’ MaxPooling â†’ Conv2D â†’ MaxPooling â†’ Flatten â†’ Dense â†’ Dropout â†’ Output
- **Output classes:** 4 (left, right, forward, stop)
- **Frameworks:** TensorFlow + Keras
- **Training time:** ~20 epochs
- **Loss function:** categorical_crossentropy

Once trained, the model is saved as `eye_direction_model.h5`.

---

## ğŸ’» Technologies Used

- Python 3
- TensorFlow & Keras
- OpenCV
- NumPy
- Socket (TCP communication)
- gTTS (Turkish Text-to-Speech)
- RPi.GPIO (motor control)

---

## ğŸš— Hardware Requirements

- Raspberry Pi 4 (4GB recommended)
- L298N Motor Driver
- 2 DC Motors
- PAM8403 Audio Amplifier
- HX 8Î© 5W Speaker
- Powerbank or USB-C power source
- MacBook or PC with webcam for training & client script

---

## ğŸ› ï¸ How to Run

1. **Train the model** on your Mac using your custom dataset:
   ```bash
   python3 eye_direction_train.py
   
2. **Run real-time gaze detection and socket sender:**
   ```bash
   python3 deep_learning.py

3. **Run Raspberry Pi motor server:**
   ```bash
   python3 motor_server.py

## ğŸ—£ï¸ Voice Feedback
The Raspberry Pi announces each command in Turkish:

"SaÄŸa dÃ¶nÃ¼yorum" â†’ Turn right

"Sola dÃ¶nÃ¼yorum" â†’ Turn left

"Ä°leri gidiyorum" â†’ Move forward

"Duruyorum" â†’ Stop

## ğŸ“œ License
This project is open-source under the MIT License.
